{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb5a756-de82-4cac-b32c-1439cb983410",
   "metadata": {},
   "source": [
    "## Установим зависимости"
   ]
  },
  {
   "cell_type": "code",
   "id": "0744c951-1366-4669-afe1-118ab51f9865",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T19:04:40.202392Z",
     "start_time": "2025-04-08T19:02:12.514287Z"
    }
   },
   "source": [
    "!pip install pip -U\n",
    "!pip install torch==2.2.* torchvision==0.17\n",
    "!pip install polygraphy==0.49.9\n",
    "!pip install tensorrt==8.6.* --extra-index-url https://pypi.nvidia.com\n",
    "!pip install onnx"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\r\n",
      "Requirement already satisfied: pip in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (24.2)\r\n",
      "Collecting pip\r\n",
      "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\r\n",
      "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\r\n",
      "\u001B[2K   \u001B[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.8/1.8 MB\u001B[0m \u001B[31m16.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: pip\r\n",
      "  Attempting uninstall: pip\r\n",
      "    Found existing installation: pip 24.2\r\n",
      "    Uninstalling pip-24.2:\r\n",
      "      Successfully uninstalled pip-24.2\r\n",
      "Successfully installed pip-25.0.1\r\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\r\n",
      "Collecting torch==2.2.*\r\n",
      "  Downloading torch-2.2.2-cp38-cp38-manylinux1_x86_64.whl.metadata (25 kB)\r\n",
      "Collecting torchvision==0.17\r\n",
      "  Downloading torchvision-0.17.0-cp38-cp38-manylinux1_x86_64.whl.metadata (6.6 kB)\r\n",
      "Requirement already satisfied: filelock in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from torch==2.2.*) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from torch==2.2.*) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from torch==2.2.*) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from torch==2.2.*) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from torch==2.2.*) (3.1.5)\r\n",
      "Requirement already satisfied: fsspec in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from torch==2.2.*) (2024.9.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from torch==2.2.*) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from torch==2.2.*) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from torch==2.2.*) (12.1.105)\r\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.*)\r\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from torch==2.2.*) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from torch==2.2.*) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from torch==2.2.*) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from torch==2.2.*) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from torch==2.2.*) (12.1.0.106)\r\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.*)\r\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from torch==2.2.*) (12.1.105)\r\n",
      "Collecting triton==2.2.0 (from torch==2.2.*)\r\n",
      "  Downloading triton-2.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\r\n",
      "Requirement already satisfied: numpy in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from torchvision==0.17) (1.24.4)\r\n",
      "Requirement already satisfied: requests in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from torchvision==0.17) (2.32.3)\r\n",
      "Collecting torch==2.2.*\r\n",
      "  Downloading torch-2.2.0-cp38-cp38-manylinux1_x86_64.whl.metadata (25 kB)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from torchvision==0.17) (10.4.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.*) (12.8.61)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from jinja2->torch==2.2.*) (2.1.5)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from requests->torchvision==0.17) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from requests->torchvision==0.17) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from requests->torchvision==0.17) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from requests->torchvision==0.17) (2025.1.31)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from sympy->torch==2.2.*) (1.3.0)\r\n",
      "Downloading torchvision-0.17.0-cp38-cp38-manylinux1_x86_64.whl (6.9 MB)\r\n",
      "\u001B[2K   \u001B[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.9/6.9 MB\u001B[0m \u001B[31m25.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading torch-2.2.0-cp38-cp38-manylinux1_x86_64.whl (755.5 MB)\r\n",
      "\u001B[2K   \u001B[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m755.5/755.5 MB\u001B[0m \u001B[31m22.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0mm eta \u001B[36m0:00:01\u001B[0m[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\r\n",
      "\u001B[2K   \u001B[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m731.7/731.7 MB\u001B[0m \u001B[31m28.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0mm eta \u001B[36m0:00:01\u001B[0m[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\r\n",
      "\u001B[2K   \u001B[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m166.0/166.0 MB\u001B[0m \u001B[31m24.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m[36m0:00:01\u001B[0m[36m0:00:01\u001B[0m:01\u001B[0m\r\n",
      "\u001B[?25hDownloading triton-2.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\r\n",
      "\u001B[2K   \u001B[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m167.9/167.9 MB\u001B[0m \u001B[31m33.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m31m32.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: triton, nvidia-nccl-cu12, nvidia-cudnn-cu12, torch, torchvision\r\n",
      "  Attempting uninstall: triton\r\n",
      "    Found existing installation: triton 3.0.0\r\n",
      "    Uninstalling triton-3.0.0:\r\n",
      "      Successfully uninstalled triton-3.0.0\r\n",
      "  Attempting uninstall: nvidia-nccl-cu12\r\n",
      "    Found existing installation: nvidia-nccl-cu12 2.20.5\r\n",
      "    Uninstalling nvidia-nccl-cu12-2.20.5:\r\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.4.1\r\n",
      "    Uninstalling torch-2.4.1:\r\n",
      "      Successfully uninstalled torch-2.4.1\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.19.1\r\n",
      "    Uninstalling torchvision-0.19.1:\r\n",
      "      Successfully uninstalled torchvision-0.19.1\r\n",
      "Successfully installed nvidia-cudnn-cu12-8.9.2.26 nvidia-nccl-cu12-2.19.3 torch-2.2.0 torchvision-0.17.0 triton-2.2.0\r\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\r\n",
      "Collecting polygraphy==0.49.9\r\n",
      "  Downloading polygraphy-0.49.9-py2.py3-none-any.whl.metadata (5.8 kB)\r\n",
      "Downloading polygraphy-0.49.9-py2.py3-none-any.whl (346 kB)\r\n",
      "Installing collected packages: polygraphy\r\n",
      "Successfully installed polygraphy-0.49.9\r\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com, https://pypi.nvidia.com\r\n",
      "Collecting tensorrt==8.6.*\r\n",
      "  Downloading https://pypi.nvidia.com/tensorrt/tensorrt-8.6.1.post1.tar.gz (18 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting tensorrt_libs==8.6.1 (from tensorrt==8.6.*)\r\n",
      "  Downloading https://pypi.nvidia.com/tensorrt-libs/tensorrt_libs-8.6.1-py2.py3-none-manylinux_2_17_x86_64.whl (824.8 MB)\r\n",
      "\u001B[2K     \u001B[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m824.8/824.8 MB\u001B[0m \u001B[31m38.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0mm eta \u001B[36m0:00:01\u001B[0m[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting tensorrt_bindings==8.6.1 (from tensorrt==8.6.*)\r\n",
      "  Downloading https://pypi.nvidia.com/tensorrt-bindings/tensorrt_bindings-8.6.1-cp38-none-manylinux_2_17_x86_64.whl (977 kB)\r\n",
      "\u001B[2K     \u001B[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m977.8/977.8 kB\u001B[0m \u001B[31m99.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: nvidia-cuda-runtime-cu12 in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from tensorrt_libs==8.6.1->tensorrt==8.6.*) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12 in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from tensorrt_libs==8.6.1->tensorrt==8.6.*) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12 in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from tensorrt_libs==8.6.1->tensorrt==8.6.*) (12.1.3.1)\r\n",
      "Building wheels for collected packages: tensorrt\r\n",
      "  Building wheel for tensorrt (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for tensorrt: filename=tensorrt-8.6.1.post1-py2.py3-none-any.whl size=17306 sha256=9e5f312954f10af7fa2ce06b90fd7b85a5de49cbfc8c30d31fccbbc64d345dc8\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_peuflji/wheels/f8/60/18/cd525ab6015e43318a18d4cc869f837c6a7c199bf24e1fbd42\r\n",
      "Successfully built tensorrt\r\n",
      "Installing collected packages: tensorrt_bindings, tensorrt_libs, tensorrt\r\n",
      "Successfully installed tensorrt-8.6.1.post1 tensorrt_bindings-8.6.1 tensorrt_libs-8.6.1\r\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\r\n",
      "Collecting onnx\r\n",
      "  Downloading onnx-1.17.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from onnx) (1.24.4)\r\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages (from onnx) (5.29.3)\r\n",
      "Downloading onnx-1.17.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\r\n",
      "\u001B[2K   \u001B[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m16.0/16.0 MB\u001B[0m \u001B[31m30.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m[36m0:00:01\u001B[0m36m0:00:01\u001B[0m:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: onnx\r\n",
      "Successfully installed onnx-1.17.0\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "e326bc7f-80e7-4ef1-a2d1-685baed4e15d",
   "metadata": {},
   "source": [
    "## Подготовим полезные функции"
   ]
  },
  {
   "cell_type": "code",
   "id": "b640a2f1-f012-417a-8dd4-3a9159d0fe21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T19:07:58.522004Z",
     "start_time": "2025-04-08T19:07:57.019787Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader \n",
    "from torchvision import transforms\n",
    "from torchvision.datasets.imagenette import Imagenette\n",
    "\n",
    "CLASSES_MAPPING = {\n",
    "    0: 0,\n",
    "    1: 217,\n",
    "    2: 848,\n",
    "    3: 491,\n",
    "    4: 497,\n",
    "    5: 566,\n",
    "    6: 569,\n",
    "    7: 571,\n",
    "    8: 574,\n",
    "    9: 701,\n",
    "}\n",
    "\n",
    "         \n",
    "def imagenette_val_dataloader(batch_size, height, width):\n",
    "    root_dir = Path(\"./imagenette\")\n",
    "    \n",
    "    dataset = Imagenette(\n",
    "        root=root_dir, \n",
    "        split=\"val\", \n",
    "        download=not root_dir.exists(),\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Resize((height, width)), \n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225], inplace=True),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "def validate(model, batch_size, height, width):\n",
    "    val_dataloder = imagenette_val_dataloader(batch_size, height, width)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        acc = []\n",
    "        for images, labels in val_dataloder:\n",
    "            output = model(images.cuda())\n",
    "            _, predicted_labels = torch.max(output, dim=1)\n",
    "            predicted_labels = predicted_labels.cpu().tolist()\n",
    "            \n",
    "            acc += [predicted_label == CLASSES_MAPPING[label] for predicted_label, label in zip(predicted_labels, labels.tolist())]\n",
    "            \n",
    "        print(f\"acc = {sum(acc) * 100 / len(acc):.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hflabs/anaconda3/envs/ds/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "6802605d-c34f-48d5-92f8-004bcd861ed1",
   "metadata": {},
   "source": [
    "## Воспроизвидите функцию для замера latency из лекции (10 баллов)"
   ]
  },
  {
   "cell_type": "code",
   "id": "76b74157-6822-461f-bf8e-5c1e31a91344",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T19:14:13.978267Z",
     "start_time": "2025-04-08T19:14:13.974092Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "def latency_benchmark(model, test_input, warmup_n=10, benchmark_n=100):\n",
    "    # Warmup\n",
    "    for _ in range(warmup_n):\n",
    "        model(test_input)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # Benchmark with synchronization\n",
    "    stats = []\n",
    "    for _ in range(benchmark_n):\n",
    "        t0 = perf_counter()\n",
    "        model(test_input)\n",
    "        torch.cuda.synchronize()\n",
    "        t1 = perf_counter()\n",
    "        stats.append(t1 - t0)\n",
    "\n",
    "    stats = np.array(stats) * 1000  # to ms\n",
    "    mean_ms = stats.mean()\n",
    "    std_ms = stats.std()\n",
    "\n",
    "    print(f\"{mean_ms:.3f}ms +- {std_ms:.3f}ms\")\n",
    "\n",
    "    assert (std_ms / mean_ms) < 0.1, \\\n",
    "        \"слишком большое отклонение в измерениях (> 10%), проверьте код, возможно стоит поднять кол-во запусков\"\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "9d41fd9d-397e-433b-939e-9fcf3e54c87b",
   "metadata": {},
   "source": [
    "## Проверяем как работает функция замера latency"
   ]
  },
  {
   "cell_type": "code",
   "id": "54543867-a23a-4fc1-8ce5-7e0ef313eae1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T19:15:18.503330Z",
     "start_time": "2025-04-08T19:14:48.520893Z"
    }
   },
   "source": [
    "from torchvision.models import mobilenet_v2\n",
    "from torchvision.models import MobileNetV2\n",
    "\n",
    "\n",
    "model = mobilenet_v2(weights=MobileNetV2).eval().cuda()\n",
    "\n",
    "# запускаем под no_grad, чтобы минимизировать потребление памяти (исключает выделение памяти под градиенты)\n",
    "with torch.no_grad():\n",
    "    latency_benchmark(\n",
    "        model, \n",
    "        torch.ones(1, 3, 640, 480, device=\"cuda\"), \n",
    "        warmup_n=10, \n",
    "        benchmark_n=10000,\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.988ms +- 0.193ms\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "c3c9f393-be5c-4d77-a6da-04c17413c093",
   "metadata": {},
   "source": [
    "## Напишите функцию для записи CUDA graph (10 баллов)\n",
    "\n",
    "Функция должна вернуть объект CUDAGraph с записанным графом, входной тензор для передачи данных и выходной тензор для копирования результатов."
   ]
  },
  {
   "cell_type": "code",
   "id": "9b80e4a8-ada9-4aba-8ff9-3a79688f67d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T19:29:19.844689Z",
     "start_time": "2025-04-08T19:29:19.838780Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "def record_CUDA_graph(model, batch_size, height, width, warmup_n=10):\n",
    "    assert torch.cuda.is_available(), \"CUDA is not available\"\n",
    "\n",
    "    model = model.eval().cuda()\n",
    "    stream = torch.cuda.Stream()\n",
    "\n",
    "    # Заранее создаём input и output буферы\n",
    "    static_input = torch.empty(batch_size, 3, height, width, device='cuda').contiguous()\n",
    "    with torch.no_grad():\n",
    "        static_output = model(static_input)\n",
    "    static_output = static_output.contiguous()\n",
    "\n",
    "    static_input.requires_grad_(False)\n",
    "    static_output.requires_grad_(False)\n",
    "\n",
    "    # Warmup (на отдельном стриме)\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.stream(stream):\n",
    "            for _ in range(warmup_n):\n",
    "                model(static_input)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # Захват графа\n",
    "    graph = torch.cuda.CUDAGraph()\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.stream(stream):\n",
    "            torch.cuda.synchronize()\n",
    "            graph.capture_begin()\n",
    "            static_output.copy_(model(static_input))\n",
    "            graph.capture_end()\n",
    "\n",
    "    return graph, static_input, static_output"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "097415bd-4e33-42dd-b38c-52486608edf1",
   "metadata": {},
   "source": [
    "## Проверяем как работает функция записи CUDA graph"
   ]
  },
  {
   "cell_type": "code",
   "id": "5bbbc31f-2bf6-483f-8152-17f9a668c04d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T19:29:21.189512Z",
     "start_time": "2025-04-08T19:29:21.114252Z"
    }
   },
   "source": [
    "bsz, ch, height, width = 1, 3, 224, 224\n",
    "\n",
    "graph, input_placeholder, output_placeholder = record_CUDA_graph(model, bsz, height, width, warmup_n=10)\n",
    "\n",
    "test_data = torch.ones(bsz, ch, height, width, device=\"cuda\")\n",
    "# запускаем под no_grad, чтобы минимизировать потребление памяти (исключает выделение памяти под градиенты)\n",
    "with torch.no_grad():\n",
    "    # запускаем исходную модель\n",
    "    model_output = model(test_data)\n",
    "    \n",
    "    # запускаем graph\n",
    "    input_placeholder.copy_(test_data)\n",
    "    graph.replay()\n",
    "    graph_output = output_placeholder.clone()\n",
    "    \n",
    "    # сравниваем выходы\n",
    "    assert torch.all(model_output == graph_output), \"выход оригинальной модели и CUDA graph не совпадают\""
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "36440acf-e0bb-43c2-ad8d-55f2dd9ce6aa",
   "metadata": {},
   "source": [
    "## Сравниваем latency оригинальной модели и CUDA graph"
   ]
  },
  {
   "cell_type": "code",
   "id": "b9183fa5-47fa-4783-980e-5245f9ced99a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T19:32:33.394236Z",
     "start_time": "2025-04-08T19:31:58.106649Z"
    }
   },
   "source": [
    "def graph_runner(input_data):\n",
    "    input_placeholder.copy_(input_data)\n",
    "    graph.replay()\n",
    "    return output_placeholder\n",
    "\n",
    "\n",
    "torch.ones(1, 3, 224, 224, device='cuda')\n",
    "# запускаем под no_grad, чтобы минимизировать потребление памяти (исключает выделение памяти под градиенты)\n",
    "with torch.no_grad():\n",
    "    latency_benchmark(\n",
    "        model, \n",
    "        test_data, \n",
    "        warmup_n=10, \n",
    "        benchmark_n=10000,\n",
    "    )\n",
    "    latency_benchmark(\n",
    "        graph_runner, \n",
    "        test_data, \n",
    "        warmup_n=10, \n",
    "        benchmark_n=10000,\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.039ms +- 0.189ms\n",
      "0.484ms +- 0.103ms\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "слишком большое отклонение в измерениях (> 10%), проверьте код, возможно стоит поднять кол-во запусков",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 16\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m     10\u001B[0m     latency_benchmark(\n\u001B[1;32m     11\u001B[0m         model, \n\u001B[1;32m     12\u001B[0m         test_data, \n\u001B[1;32m     13\u001B[0m         warmup_n\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, \n\u001B[1;32m     14\u001B[0m         benchmark_n\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10000\u001B[39m,\n\u001B[1;32m     15\u001B[0m     )\n\u001B[0;32m---> 16\u001B[0m     \u001B[43mlatency_benchmark\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgraph_runner\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwarmup_n\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbenchmark_n\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[9], line 25\u001B[0m, in \u001B[0;36mlatency_benchmark\u001B[0;34m(model, test_input, warmup_n, benchmark_n)\u001B[0m\n\u001B[1;32m     21\u001B[0m std_ms \u001B[38;5;241m=\u001B[39m stats\u001B[38;5;241m.\u001B[39mstd()\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmean_ms\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mms +- \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstd_ms\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mms\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m (std_ms \u001B[38;5;241m/\u001B[39m mean_ms) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0.1\u001B[39m, \\\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mслишком большое отклонение в измерениях (> 10\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m), проверьте код, возможно стоит поднять кол-во запусков\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mAssertionError\u001B[0m: слишком большое отклонение в измерениях (> 10%), проверьте код, возможно стоит поднять кол-во запусков"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "d5e146e6-057f-48ea-8da5-3230ea17279a",
   "metadata": {},
   "source": [
    "## Экспортируйте torchvision модель в onnx файл (10 баллов)\n",
    "\n",
    "Для тестов нам потребуется 2 варианта onnx:\n",
    "1. все входные оси фиксированны (1, 3, 224, 224). Файл назовите \"my-model-ssss.onnx\".\n",
    "1. размер батча, высота и ширина динамические, количество входных каналов фиксированное (1, 3, height, width). Файл назовите \"my-model-dsdd.onnx\".\n",
    "\n",
    "Имя входа должно быть \"x\", имя выхода \"output\".\n",
    "\n",
    "Докуметацию по экспорту в onnx можно почитать [тут](https://pytorch.org/docs/stable/onnx_torchscript.html#torch.onnx.export)."
   ]
  },
  {
   "cell_type": "code",
   "id": "be2ea214-190a-437e-ac97-9c4cb9adf94f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T19:51:14.940105Z",
     "start_time": "2025-04-08T19:51:14.097258Z"
    }
   },
   "source": [
    "model.cpu()\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "# 1)\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"my-model-ssss.onnx\",\n",
    "    input_names=[\"x\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes=None,\n",
    "    opset_version=11\n",
    ")\n",
    "\n",
    "# 2)\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"my-model-dsdd.onnx\",\n",
    "    input_names=[\"x\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"x\": {0: \"batch\", 2: \"height\", 3: \"width\"},\n",
    "        \"output\": {0: \"batch\"} \n",
    "    },\n",
    "    opset_version=11\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "c0fd400f-21ba-42db-9af6-0e43218111f4",
   "metadata": {},
   "source": [
    "## Скомпилируйте простейший вариант модели без динамических осей"
   ]
  },
  {
   "cell_type": "code",
   "id": "54745689-3830-46f8-8d60-9b66553d3d34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:00:24.408446Z",
     "start_time": "2025-04-08T20:00:00.610465Z"
    }
   },
   "source": [
    "from polygraphy.backend.trt import CreateConfig\n",
    "from polygraphy.backend.trt import engine_from_network\n",
    "from polygraphy.backend.trt import NetworkFromOnnxPath\n",
    "from polygraphy.backend.trt import save_engine\n",
    "\n",
    "\n",
    "model_ssss = NetworkFromOnnxPath(\"my-model-ssss.onnx\")\n",
    "config = CreateConfig()\n",
    "\n",
    "engine = engine_from_network(model_ssss, config=config)\n",
    "save_engine(engine, path=\"my-model-ssss.engine\");"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W] 'colored' module is not installed, will not use colors when logging. To enable colors, please install the 'colored' module: python3 -m pip install colored\n",
      "[I] Configuring with profiles:[\n",
      "        Profile 0:\n",
      "            {x [min=[1, 3, 224, 224], opt=[1, 3, 224, 224], max=[1, 3, 224, 224]]}\n",
      "    ]\n",
      "[I] Building engine with configuration:\n",
      "    Flags                  | []\n",
      "    Engine Capability      | EngineCapability.DEFAULT\n",
      "    Memory Pools           | [WORKSPACE: 16080.25 MiB, TACTIC_DRAM: 16080.25 MiB]\n",
      "    Tactic Sources         | [CUBLAS, CUBLAS_LT, CUDNN, EDGE_MASK_CONVOLUTIONS, JIT_CONVOLUTIONS]\n",
      "    Profiling Verbosity    | ProfilingVerbosity.DETAILED\n",
      "    Preview Features       | [FASTER_DYNAMIC_SHAPES_0805, DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n",
      "[I] Finished engine building in 19.525 seconds\n",
      "[I] Saving engine to my-model-ssss.engine\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "f7a5ab1e-a3f6-4407-bf35-b0880522a1b0",
   "metadata": {},
   "source": [
    "## Проверти что точность не просела\n",
    "\n",
    "**ВАЖНО:**<br>\n",
    "Опция ``copy_outputs_to_host=False`` позволяет пропустить копирование данных с GPU на CPU.<br>\n",
    "Вместо numpy array мы получим, torch.Tensor, что бывает очень полезно и экономит время на копировании."
   ]
  },
  {
   "cell_type": "code",
   "id": "fc5dd258-b355-44e0-b150-f171eff5ee2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:02:06.588819Z",
     "start_time": "2025-04-08T20:00:31.023458Z"
    }
   },
   "source": [
    "from polygraphy.backend.trt import TrtRunner\n",
    "\n",
    "\n",
    "with TrtRunner(engine) as trt_runner:\n",
    "    def validation_trt_runner(input_data):\n",
    "        # пропустим копирование на CPU copy_outputs_to_host=False\n",
    "        output = trt_runner.infer(feed_dict={\"x\": input_data}, copy_outputs_to_host=False)\n",
    "        return output['output']\n",
    "\n",
    "    validate(validation_trt_runner, batch_size=1, height=224, width=224)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz to imagenette/imagenette2.tgz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1557161267/1557161267 [01:14<00:00, 21021119.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting imagenette/imagenette2.tgz to imagenette\n",
      "acc = 79.24%\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "5b4ec542-1f1b-4d3b-a4a4-b2b35f1c53de",
   "metadata": {},
   "source": [
    "## Сделайте замер latency с помощью ранее написанной функции latency_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "id": "637d7f0a-de26-4d6c-bc4a-937f1456de6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:05:03.936683Z",
     "start_time": "2025-04-08T20:04:52.145317Z"
    }
   },
   "source": [
    "with TrtRunner(engine) as trt_runner:\n",
    "    def validation_trt_runner(input_data):\n",
    "        # пропустим копирование на CPU copy_outputs_to_host=False\n",
    "        output = trt_runner.infer(feed_dict={\"x\": input_data}, copy_outputs_to_host=False)\n",
    "        return output['output']\n",
    "\n",
    "    latency_benchmark(validation_trt_runner, test_input=torch.ones(1, 3, 224, 224), warmup_n=10, benchmark_n=10000)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.172ms +- 1.625ms\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "слишком большое отклонение в измерениях (> 10%), проверьте код, возможно стоит поднять кол-во запусков",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m     output \u001B[38;5;241m=\u001B[39m trt_runner\u001B[38;5;241m.\u001B[39minfer(feed_dict\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m\"\u001B[39m: input_data}, copy_outputs_to_host\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m----> 7\u001B[0m \u001B[43mlatency_benchmark\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalidation_trt_runner\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mones\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m224\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m224\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwarmup_n\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbenchmark_n\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[9], line 25\u001B[0m, in \u001B[0;36mlatency_benchmark\u001B[0;34m(model, test_input, warmup_n, benchmark_n)\u001B[0m\n\u001B[1;32m     21\u001B[0m std_ms \u001B[38;5;241m=\u001B[39m stats\u001B[38;5;241m.\u001B[39mstd()\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmean_ms\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mms +- \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstd_ms\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mms\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m (std_ms \u001B[38;5;241m/\u001B[39m mean_ms) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0.1\u001B[39m, \\\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mслишком большое отклонение в измерениях (> 10\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m), проверьте код, возможно стоит поднять кол-во запусков\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mAssertionError\u001B[0m: слишком большое отклонение в измерениях (> 10%), проверьте код, возможно стоит поднять кол-во запусков"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "597295b4-6b63-4119-9ada-315c02a351ba",
   "metadata": {},
   "source": [
    "## Теперь на основе примера выше скомпилируйте модель с динамическим batch size в диапазоне [1, 64] (5 баллов)\n",
    "**ВАЖНО:**<br>\n",
    "Как задать конфиг для динамических осей? Читаем доку [тут](https://docs.nvidia.com/deeplearning/tensorrt/polygraphy/docs/backend/trt/profile.html#optimization-profile) и добавляем профиль в config."
   ]
  },
  {
   "cell_type": "code",
   "id": "7128ffc4-20b7-4a48-8a65-0753f727ad9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:12:39.308716Z",
     "start_time": "2025-04-08T20:12:18.431549Z"
    }
   },
   "source": [
    "from polygraphy.backend.trt import Profile\n",
    "model_dsdd = NetworkFromOnnxPath(\"my-model-dsdd.onnx\")\n",
    "\n",
    "profile = Profile()\n",
    "profile.add(\"x\", min=(1, 3, 224, 224), opt=(8, 3, 224, 224), max=(64, 3, 224, 224))\n",
    "\n",
    "config = CreateConfig(profiles=[profile])\n",
    "engine = engine_from_network(model_dsdd, config=config)\n",
    "save_engine(engine, path=\"my-model-dsdd.engine\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I] Configuring with profiles:[\n",
      "        Profile 0:\n",
      "            {x [min=(1, 3, 224, 224), opt=(8, 3, 224, 224), max=(64, 3, 224, 224)]}\n",
      "    ]\n",
      "[I] Building engine with configuration:\n",
      "    Flags                  | []\n",
      "    Engine Capability      | EngineCapability.DEFAULT\n",
      "    Memory Pools           | [WORKSPACE: 16080.25 MiB, TACTIC_DRAM: 16080.25 MiB]\n",
      "    Tactic Sources         | [CUBLAS, CUBLAS_LT, CUDNN, EDGE_MASK_CONVOLUTIONS, JIT_CONVOLUTIONS]\n",
      "    Profiling Verbosity    | ProfilingVerbosity.DETAILED\n",
      "    Preview Features       | [FASTER_DYNAMIC_SHAPES_0805, DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n",
      "[I] Finished engine building in 17.210 seconds\n",
      "[I] Saving engine to my-model-dsdd.engine\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorrt_bindings.tensorrt.ICudaEngine at 0x7d264fb2aeb0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "id": "c1547209-eac1-4a68-9d18-53a1411aa4d5",
   "metadata": {},
   "source": [
    "## Проверте что точность не просела и замерте latency для batch size 1 и 64"
   ]
  },
  {
   "cell_type": "code",
   "id": "ace62c09-5d23-47bc-86ec-6ffea03ccf8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:17:31.145083Z",
     "start_time": "2025-04-08T20:17:02.428739Z"
    }
   },
   "source": [
    "with TrtRunner(engine) as trt_runner:\n",
    "    def validation_trt_runner(input_data):\n",
    "        output = trt_runner.infer(feed_dict={\"x\": input_data}, copy_outputs_to_host=False)\n",
    "        return output['output']\n",
    "\n",
    "    validate(validation_trt_runner, batch_size=1, height=224, width=224)\n",
    "    latency_benchmark(validation_trt_runner, test_input=torch.ones(1, 3, 224, 224), warmup_n=10, benchmark_n=10000)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 79.24%\n",
      "1.343ms +- 1.890ms\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "слишком большое отклонение в измерениях (> 10%), проверьте код, возможно стоит поднять кол-во запусков",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[39], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      6\u001B[0m validate(validation_trt_runner, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, height\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m224\u001B[39m, width\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m224\u001B[39m)\n\u001B[0;32m----> 7\u001B[0m \u001B[43mlatency_benchmark\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalidation_trt_runner\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mones\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m224\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m224\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwarmup_n\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbenchmark_n\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[9], line 25\u001B[0m, in \u001B[0;36mlatency_benchmark\u001B[0;34m(model, test_input, warmup_n, benchmark_n)\u001B[0m\n\u001B[1;32m     21\u001B[0m std_ms \u001B[38;5;241m=\u001B[39m stats\u001B[38;5;241m.\u001B[39mstd()\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmean_ms\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mms +- \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstd_ms\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mms\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m (std_ms \u001B[38;5;241m/\u001B[39m mean_ms) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0.1\u001B[39m, \\\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mслишком большое отклонение в измерениях (> 10\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m), проверьте код, возможно стоит поднять кол-во запусков\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mAssertionError\u001B[0m: слишком большое отклонение в измерениях (> 10%), проверьте код, возможно стоит поднять кол-во запусков"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:20:52.243878Z",
     "start_time": "2025-04-08T20:17:33.291181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with TrtRunner(engine) as trt_runner:\n",
    "    def validation_trt_runner(input_data):\n",
    "        output = trt_runner.infer(feed_dict={\"x\": input_data}, copy_outputs_to_host=False)\n",
    "        return output['output']\n",
    "\n",
    "    validate(validation_trt_runner, batch_size=64, height=224, width=224)\n",
    "    latency_benchmark(validation_trt_runner, test_input=torch.ones(64, 3, 224, 224), warmup_n=10, benchmark_n=10000) # не вместилось видимо"
   ],
   "id": "499b7469595ebf1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 79.24%\n",
      "18.727ms +- 2.697ms\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "слишком большое отклонение в измерениях (> 10%), проверьте код, возможно стоит поднять кол-во запусков",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[40], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      6\u001B[0m validate(validation_trt_runner, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m, height\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m224\u001B[39m, width\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m224\u001B[39m)\n\u001B[0;32m----> 7\u001B[0m \u001B[43mlatency_benchmark\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalidation_trt_runner\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mones\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m224\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m224\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwarmup_n\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbenchmark_n\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[9], line 25\u001B[0m, in \u001B[0;36mlatency_benchmark\u001B[0;34m(model, test_input, warmup_n, benchmark_n)\u001B[0m\n\u001B[1;32m     21\u001B[0m std_ms \u001B[38;5;241m=\u001B[39m stats\u001B[38;5;241m.\u001B[39mstd()\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmean_ms\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mms +- \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstd_ms\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mms\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m (std_ms \u001B[38;5;241m/\u001B[39m mean_ms) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0.1\u001B[39m, \\\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mслишком большое отклонение в измерениях (> 10\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m), проверьте код, возможно стоит поднять кол-во запусков\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mAssertionError\u001B[0m: слишком большое отклонение в измерениях (> 10%), проверьте код, возможно стоит поднять кол-во запусков"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "113074c3-e8d0-48c4-b0a7-dd059d1cc772",
   "metadata": {},
   "source": [
    "## Скомпилируйте квантованный вариант engine c динамическим batch size [1, 64] (15 балов)\n",
    "\n",
    "Для этого вам потребуется на основе кода ``imagenette_val_dataloader`` сделать свой калибратор. Документацию на калибратор можно найти [тут](https://docs.nvidia.com/deeplearning/tensorrt/polygraphy/docs/backend/trt/calibrator.html#polygraphy.backend.trt.calibrator.Calibrator)."
   ]
  },
  {
   "cell_type": "code",
   "id": "d031fee2-3a94-476d-8c7e-03d8ca278b09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:46:04.051200Z",
     "start_time": "2025-04-09T08:45:00.405771Z"
    }
   },
   "source": [
    "from polygraphy.backend.trt import Calibrator\n",
    "def imagenette_calibration_data(dataloader):\n",
    "    for images, labels in dataloader:\n",
    "        images_np = np.ascontiguousarray(images.numpy()).astype(np.float32)        \n",
    "        yield {\"x\": images_np}\n",
    "\n",
    "val_dataloader = imagenette_val_dataloader(batch_size=8, height=224, width=224)\n",
    "calibration_gen = imagenette_calibration_data(val_dataloader)\n",
    "calibrator = Calibrator(\n",
    "    data_loader=calibration_gen,\n",
    "    cache=\"calibration.cache\"\n",
    ")\n",
    "\n",
    "model_dsdd = NetworkFromOnnxPath(\"my-model-dsdd.onnx\")\n",
    "\n",
    "profile = Profile()\n",
    "profile.add(\"x\", min=(1,3,224,224), opt=(8,3,224,224), max=(64,3,224,224))\n",
    "\n",
    "config = CreateConfig(\n",
    "    profiles=[profile],\n",
    "    int8=True,\n",
    "    calibrator=calibrator\n",
    ")\n",
    "\n",
    "engine = engine_from_network(model_dsdd, config=config)\n",
    "save_engine(engine, path=\"my-model-dsdd.int8.engine\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I] Configuring with profiles:[\n",
      "        Profile 0:\n",
      "            {x [min=(1, 3, 224, 224), opt=(8, 3, 224, 224), max=(64, 3, 224, 224)]}\n",
      "    ]\n",
      "[I] Using calibration profile: {x [min=(1, 3, 224, 224), opt=(8, 3, 224, 224), max=(64, 3, 224, 224)]}\n",
      "[I] Building engine with configuration:\n",
      "    Flags                  | [INT8]\n",
      "    Engine Capability      | EngineCapability.DEFAULT\n",
      "    Memory Pools           | [WORKSPACE: 16080.25 MiB, TACTIC_DRAM: 16080.25 MiB]\n",
      "    Tactic Sources         | [CUBLAS, CUBLAS_LT, CUDNN, EDGE_MASK_CONVOLUTIONS, JIT_CONVOLUTIONS]\n",
      "    Profiling Verbosity    | ProfilingVerbosity.DETAILED\n",
      "    Preview Features       | [FASTER_DYNAMIC_SHAPES_0805, DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805]\n",
      "    Calibrator             | Calibrator(<generator object imagenette_calibration_data at 0x7d26797cc200>, cache='calibration.cache', BaseClass=<class 'tensorrt_bindings.tensorrt.IInt8EntropyCalibrator2'>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[!] Input tensor: x | Received incompatible shape: (5, 3, 224, 224).\n",
      "    Note: Expected a shape compatible with: BoundedShape([8, 3, 224, 224], min=None, max=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I] Saving calibration cache to calibration.cache\n",
      "[I] Finished engine building in 63.567 seconds\n",
      "[I] Saving engine to my-model-dsdd.int8.engine\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorrt_bindings.tensorrt.ICudaEngine at 0x7d2679711e70>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "id": "320c7a06-117e-476d-b592-afa9c578b42d",
   "metadata": {},
   "source": [
    "## Проверте что точность не просела и замерте latency для batch size 1 и 64"
   ]
  },
  {
   "cell_type": "code",
   "id": "d555e6e4-533e-4252-8195-8266d7f67e18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:51:02.444075Z",
     "start_time": "2025-04-09T08:50:37.611685Z"
    }
   },
   "source": [
    "with TrtRunner(engine) as trt_runner:\n",
    "    def validation_trt_runner(input_data):\n",
    "        output = trt_runner.infer(feed_dict={\"x\": input_data}, copy_outputs_to_host=False)\n",
    "        return output['output']\n",
    "\n",
    "    validate(validation_trt_runner, batch_size=1, height=224, width=224)\n",
    "    latency_benchmark(validation_trt_runner, test_input=torch.ones(1, 3, 224, 224), warmup_n=10, benchmark_n=10000)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 77.53%\n",
      "1.060ms +- 1.308ms\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "слишком большое отклонение в измерениях (> 10%), проверьте код, возможно стоит поднять кол-во запусков",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[55], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      6\u001B[0m validate(validation_trt_runner, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, height\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m224\u001B[39m, width\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m224\u001B[39m)\n\u001B[0;32m----> 7\u001B[0m \u001B[43mlatency_benchmark\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalidation_trt_runner\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mones\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m224\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m224\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwarmup_n\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbenchmark_n\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[9], line 25\u001B[0m, in \u001B[0;36mlatency_benchmark\u001B[0;34m(model, test_input, warmup_n, benchmark_n)\u001B[0m\n\u001B[1;32m     21\u001B[0m std_ms \u001B[38;5;241m=\u001B[39m stats\u001B[38;5;241m.\u001B[39mstd()\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmean_ms\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mms +- \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstd_ms\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mms\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m (std_ms \u001B[38;5;241m/\u001B[39m mean_ms) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0.1\u001B[39m, \\\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mслишком большое отклонение в измерениях (> 10\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m), проверьте код, возможно стоит поднять кол-во запусков\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mAssertionError\u001B[0m: слишком большое отклонение в измерениях (> 10%), проверьте код, возможно стоит поднять кол-во запусков"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T08:52:51.461532Z",
     "start_time": "2025-04-09T08:52:05.374381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with TrtRunner(engine) as trt_runner:\n",
    "    def validation_trt_runner(input_data):\n",
    "        output = trt_runner.infer(feed_dict={\"x\": input_data}, copy_outputs_to_host=False)\n",
    "        return output['output']\n",
    "\n",
    "    validate(validation_trt_runner, batch_size=32, height=224, width=224)\n",
    "    latency_benchmark(validation_trt_runner, test_input=torch.ones(32, 3, 224, 224), warmup_n=10, benchmark_n=10000)"
   ],
   "id": "c12a4cf881e5f2e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 77.53%\n",
      "3.596ms +- 3.578ms\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "слишком большое отклонение в измерениях (> 10%), проверьте код, возможно стоит поднять кол-во запусков",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[56], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      6\u001B[0m validate(validation_trt_runner, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m, height\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m224\u001B[39m, width\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m224\u001B[39m)\n\u001B[0;32m----> 7\u001B[0m \u001B[43mlatency_benchmark\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalidation_trt_runner\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mones\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m224\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m224\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwarmup_n\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbenchmark_n\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[9], line 25\u001B[0m, in \u001B[0;36mlatency_benchmark\u001B[0;34m(model, test_input, warmup_n, benchmark_n)\u001B[0m\n\u001B[1;32m     21\u001B[0m std_ms \u001B[38;5;241m=\u001B[39m stats\u001B[38;5;241m.\u001B[39mstd()\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmean_ms\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mms +- \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstd_ms\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mms\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m (std_ms \u001B[38;5;241m/\u001B[39m mean_ms) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0.1\u001B[39m, \\\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mслишком большое отклонение в измерениях (> 10\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m), проверьте код, возможно стоит поднять кол-во запусков\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mAssertionError\u001B[0m: слишком большое отклонение в измерениях (> 10%), проверьте код, возможно стоит поднять кол-во запусков"
     ]
    }
   ],
   "execution_count": 56
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
